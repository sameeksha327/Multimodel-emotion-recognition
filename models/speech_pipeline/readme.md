Steps performed:

Loaded audio files from the dataset

Extracted acoustic features such as MFCC

Preprocessed features and normalized them

Split data into training and testing sets

Built a deep learning model for classification

Trained the model using categorical cross-entropy loss

Evaluated the model using accuracy metric

Result:

Speech Model Accuracy: 96%

This shows that acoustic features effectively capture emotional patterns.
